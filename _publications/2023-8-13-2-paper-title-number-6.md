---
title: "PRSNet: A Masked Self-Supervised Learning Pedestrian Re-Identification Method"
collection: publications
permalink: /publication/2023-8-13-paper-title-number-6
excerpt: 'In recent years, self-supervised learning has attracted widespread academic debate and addressed many of the key issues of computer vision. The present research focus is on how to construct a good agent task that allows for improved network learning of advanced semantic information on images so that model reasoning is accelerated during pre-training of the current task. In order to solve the problem that existing feature extraction networks are pre-trained on the ImageNet dataset and cannot extract the fine-grained information in pedestrian images well, and the existing pre-task of contrast self-supervised learning may destroy the original properties of pedestrian images, this paper designs a pre-task of mask reconstruction to obtain a pre-training model with strong robustness and uses it for the pedestrian re-identification task. The training optimization of the network is performed by improving the triplet loss based on the centroid, and the mask image is added as an additional sample to the loss calculation, so that the network can better cope with the pedestrian matching in practical applications after the training is completed. This method achieves about 5% higher mAP on Marker1501 and CUHK03 data than existing self-supervised learning pedestrian re-identification methods, and about 1% higher for Rank1, and ablation experiments are conducted to demonstrate the feasibility of this method. Our model code is located at this https URL.'
date: 2022-3-11
venue: 'Conference 1'
paperurl: 'https://arxiv.org/pdf/2303.06330.pdf'
citation: 'Xiao, Zhijie et al. “PRSNet: A Masked Self-Supervised Learning Pedestrian Re-Identification Method.” ArXiv abs/2303.06330 (2023): n. pag.'
---
[Download paper here](https://arxiv.org/pdf/2303.06330.pdf)

Recommended citation: Xiao, Zhijie et al. “PRSNet: A Masked Self-Supervised Learning Pedestrian Re-Identification Method.” ArXiv abs/2303.06330 (2023): n. pag.
